# triton-inference-helm
NVIDIA Triton Inference Server on Kubernetes with a minimal model repo (e.g., ResNet50 ONNX), Helm chart, HPA, and Locust load test. Includes Prometheus/Grafana metrics and a ready-to-import dashboard. Demo: dynamic batching and throughput/latency trade-offs.
